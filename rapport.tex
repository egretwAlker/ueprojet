\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{angles, quotes}
\usetikzlibrary{calc}
\usepackage{pgfplots}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{mdframed}
\usepackage[margin=2cm]{geometry}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    }
\usepackage{biblatex}
\addbibresource{refs.bib}
\usepackage{indentfirst}
\setlength{\parindent}{0.5cm}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Com}{Com}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ker}{Ker}
\newtheorem{theorem}{Theoreme}
\newtheorem*{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Exemple}
\newtheorem{tactic}{Tactic}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbb{I}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\Cn}{\mathscr{C}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\RM}[1]{\paragraph{RM} #1}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\id}{id}
\title{Textual representations and semantic embeddings: an application for sentiment/essay analysis}
\author{MARIE Clément, SAMAHA Elio, XIA Tianxiang}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Introduction - Motivation}
We were given several datasets from the website \textit{https://www.trictrac.net/}, which sells a variety of board games. This website collects data for every article, such as reviews and grades posted by the users. Our task was to treat and analyze this data to gain valuable insights from it. One purpose was to classify these reviews and predict the sentiment (positive or negative).

This is a real-world application as the company could automatically gain insight on products by correlating words with grades. Utilizing word vector representation and dimensionality reduction helps establish meaningful associations between words and the sentiment expressed in reviews. By exploring the represented space, the company could identify trends in customer preferences, potentially increasing sales.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\linewidth]{tric_trac_pic.png}
    \caption{Tric Trac webpage : on the left of a star we have a grade, and below is the comment}
\end{figure}

The features X represents the \textit{comment} column, and the target variable y is \textit{grade}. In this report, we aim to uncover valuable insights from these reviews and effectively capture and quantify the sentiments conveyed. We will discuss our methodology, data processing approaches, and the potential of our findings.

\newpage
\section{Descriptive statistics of the \textit{grades} feature}

We are starting by looking at the grades data since it will be the criterion to classify reviews as positive or negative.
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{hist_plot_count.png}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{dist_plot_count.png}
  \end{minipage}
  \caption{Grade distribution}
\end{figure}

On the histogram on the right plot, 
we can notice that the majority of the grades are actually integers. Removing float numbers (or rounding to closest integer) could possibly simplify the model.
On the left plot, we can notice that the majority of reviews are actually positive.

It is cautious to already say it could introduce some challenges. Indeed, since grades are quite imbalanced, the model can become biased towards positive reviews. The model may tend to predict positive reviews more frequently, leading to lower accuracy for negative ones and an overall imbalance in predictions. Also, since there are fewer samples from negative reviews, the model may not have enough representative examples to learn the patterns and characteristics of the negative class.

\section{Text documents to vectors}

Text documents, in their raw form, consist of unstructured data that cannot be directly processed by machine learning algorithms. To effectively analyze and derive insights from text, it is necessary to transform the textual information into a numerical representation known as vectors because they provide a structured and quantifiable representation of textual information.

Therefore, since the features of the dataset represent the \textit{comment} feature, we will tackle ways to treat the data.
\subsection{Preprocessing}

Raw text data might contain unwanted or unimportant text due to which our results might not give efficient accuracy and might make it hard to understand and analyze. So, proper pre-processing must be done on raw data.

We had to remove punctuation, URLs, numbers, accents, and stopwords.
Stopwords are commonly used words in a language that are considered to have little or no significance in determining the meaning of a text.
`not' is not a stopword, because it might indicate the opposite sentiment.

We also applied Stemming which reduces words to their root forms. It minimizes the confusion around words that have similar meanings and lowers the complexity of the input space. However, it comes at the cost of throwing information away.

For example:
\begin{verbatim}
J'avais peur que les extensions s'éssouflent à terme...
->
['peur', 'extens', 'éssouflent', 'term', ...]
\end{verbatim}

\subsection{One Hot Encoding}

One Hot Encoding is a vector representation of a word in the vocabulary, i.e.\ the unique list of all the words appearing in the documents.
If the vocabulary size is n, each word in the vocabulary is therefore represented as a vector of size n. It takes binary values: 1 for the corresponding word and 0 otherwise.

\paragraph{On the implementation:}

We can represent the set of sentences as a tensor of shape (a, b, c) ie a matrices (number of sentences) of shape (b,c) where b represents the number of words in the sentence and c the vocabulary size.

\paragraph{Advantages:}

\begin{itemize}
  \item Intuitive and easy to implement
\end{itemize}

\paragraph{Inconvenients:}

\begin{itemize}
  \item Increase in dimensionality: a large vocabulary size implies a large number of columns, taking more memory size, which results in an increase in computational cost. + The matrix is sparse.
  \item Every vector sits in the orthogonal vector space so vectors are perpendicular to each other and are considered independent to each other, which is rarely the case.
  \item High chance of multi-collinearity due to dummy variables, which might affect the performance of the model (cf. Dummy Variable Trap)
\end{itemize}

\subsection{Bag of Words}

The Bag of Words (BoW) technique is a commonly used method for representing text data in natural language processing. It treats each document or sentence as a collection of words without considering the order or grammar, focusing only on the frequency of words. BoW represents text data as a sparse matrix, where each row corresponds to a document or sentence, and each column represents a unique word in the vocabulary.

\paragraph{Implementation Details}

In the BoW representation, the set of sentences is transformed into a matrix, where each row corresponds to a document or sentence, and each column represents a unique word in the vocabulary. The value in each cell of the matrix indicates the frequency or occurrence of the corresponding word in the document.

\paragraph{Advantages}

\begin{itemize}
\item \textbf{Intuitive and easy to implement}: The BoW technique provides a straightforward representation of text data, where words are treated as independent features, and their frequencies capture some information about the documents.
\end{itemize}

\paragraph{Disadvantages}

\begin{itemize}
\item \textbf{Increase in dimensionality}: One drawback of the BoW technique is the increase in dimensionality. As the vocabulary size grows, the number of columns in the matrix increases, occupying more memory space and resulting in higher computational costs. Additionally, the resulting matrix is sparse, containing mostly zeros.
\item \textbf{Loss of word order and grammar}: By disregarding the order and grammar of words, the BoW technique may lose important linguistic information present in the text data.
\item \textbf{Lack of semantic meaning}: BoW representation treats each word independently, ignoring the semantic relationships between words, which can limit its ability to capture the meaning and context of the text.
\end{itemize}


\subsection{tf-idf}

The BoW (Bag of Words) model assumes that the importance of a term is directly proportional to
the number of its appearance in the document, this can easily be misleading
when the most common words are `stopwords'. (But it really depends on
the algorithm that we are going to use afterwards, if we do perceptron, it wouldn't matter.)

But the BoW gives vectors with integer features, which may be favored by some algorithms,
like Naive Bayes below. This being said, the tf-idf embedding gives vectors with features in $\mathbb{Q}$,
which can be dilated to integers.

tf-idf (term frequency-inverse document frequency) first considers
the whole of a corpus (for the terminology, we call the set of comments a corpus,
each comment a document, every word a term), it assumes a term too frequent in the corpus has little
information. Then it considers the importance of a term in a document
as the frequency of the term in this document times a scalar
representing its information in the corpus. The idf is actually an estimation
of Shannon information of a term.

$$
\begin{aligned}
\mathrm{tfidf}(\mathrm{term}) & = \mathrm{tf}(\mathrm{term}) \times \mathrm{idf}(\mathrm{term}) \\
\mathrm{tf}(\mathrm{term}) & = \frac{\# \text { of times term appears in document }}{\# \text { of terms in document }} \\
\mathrm{idf}(\mathrm{term}) & =\ln \left(\frac{\# \text { of documents }}{\# \text { of documents in corpus with term }}\right)
\end{aligned}
$$

Other possible choices:

\begin{itemize}
  \item A term can be many (successive) words (n-gram) (e.g. to
  take into account negations before a word)
  \item The exact formulae can be changed while the same idea remains
\end{itemize}

\paragraph{On the implementation} \verb|for| is slow in python, do use
functions in numpy instead; we use sparse matrices as data structure to
gain memory and boost speed sometimes.

\paragraph{Observations}
\begin{itemize}
  \item Preprocessing is important to reduce the dimension
\end{itemize}

\section{Classification}

\subsection{Naive Bayes}

The Naive Bayes algorithm does not require text representations such as one-hot encoding. It instead utilizes count vectorization (i.e. bag of words) to represent documents. Count vectorization preserves the word frequencies, allowing the algorithm to consider the importance of words in expressing sentiment.

Naive Bayes is a probabilistic classification algorithm.
It is called naive because it assumes that each input variable is independent.

We are looking for:

$$ \max_{y}\mathbb{P}(Y =y \mid X=(x_1, ..., x_n)) $$

using the Naive Bayes formula: $\mathbb{P}(Y \mid X) = \frac{\mathbb{P}(X\mid Y)\cdot \mathbb{P}(Y)}{\mathbb{P}(X)} = \frac{\mathbb{P}(Y)\prod_{i}^{}\mathbb{P}(X_i\mid Y)}{\mathbb{P}(X)}$
by independence,
where $\mathbb{P}(X\mid Y)$ is the likelihood, $\mathbb{P}(X)$ is the evidence, $\mathbb{P}(Y \mid X)$ is the posterior and $\mathbb{P}(Y)$ is the prior.

More precisely, we used Multinomial Naive Bayes for our task because it is suitable for discrete and count-based features, such as word frequencies in text data. The count vectorization of words aligns well with the assumptions of this model. It allows the model to estimate the probability of sentiment classes based on the frequency distribution of words.

\paragraph{Advantages} \begin{itemize}
\item Even though the independence assumption is rarely true, the model is still effective
\item Handles high dimensional data well
\end{itemize}

\paragraph{Inconvenients} \begin{itemize}
\item Estimated probability is often inaccurate because of the naive assumption
\end{itemize}

\subsection{Distances}

Before introducing the k-nearest neighbors algorithm,
it is important to note first that the distances to measure the similarities between documents we use for the algorithms are vital.

We can use Euclidean distance, or we can use the distance concerning
only the angle between 2 vectors (so the `difference of lengths' of documents
is ignored).

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
$$
\begin{aligned}
&\text{(cosine similarity)}S(u, v) :=
\cos (\theta)=\frac{u \cdot v}{\|u\|\|v\|}\\
&\text{(cosine distance)}D(u, v) :=1-S(u, v)
\end{aligned}
$$
\end{minipage}
\centering
\begin{minipage}{0.49\textwidth}
\input{dist.tex}
\end{minipage}
\end{minipage}

We see that cosine distance is better in nlp : no curse of dimensionality (the Euclidean
distance tends to be the same for every pair of vectors when the dimension grows).

\subsection{K-Nearest neighbours}
\label{subset:KNN}

\paragraph{Algorithm} One predicts the information associated with a vector by the majority
of information associated with its neighbors (within k nearest).
We use tf-idf embedding here.
By simple experience
and reasoning (the length doesn't affect the positivity) as well, we chose cosine distance.

For comments of grades 4 - 7, they are more or less neutral, we can't
say if it is definitely positive or negative as human-being, so less chance
for our more or less naïve algorithm. For this reason, we chose
to do our algorithm on comments of extremities.

We add a parameter to our algorithm: \textbf{balance}.
balance is a float >= 1 meaning that we trim the list of data of one label
of more quantity to size = balance$\times$(number of data with another label).
This is to solve the problem of disproportional labeled data.

\paragraph{On the implementation} \begin{itemize}
  \item We favor the nearer information when there is a tie.
  \item Cosine distance works much better than Euclidean distance.
  \item When the data is disproportionally labeled, we need to balance the data to ensure performance for the prediction of each label.
  \item We sometimes use implemented functions to improve
  efficacity after having understood the method and
  implemented it ourselves.
\end{itemize}

\paragraph{Positive} \begin{itemize}
  \item Easy to implement
\end{itemize}

\paragraph{Negative} \begin{itemize}
  \item Supervised
  \item $\Theta(n)$ for each prediction ($n$ the size of training data), which is
  very slow when we want to use big training data to ensure better prediction.
\end{itemize}

\section{Evaluation of models - Results}

\subsection{Model evaluation}

First of all, they split the labeled dataset into training and testing subsets. The model is trained on the training data and then evaluated on the testing data to measure its performance, in order to assess its capacity to generalize predictions to unknown data.

Then, we used cross-validation for each model. It divides the data into parts, trains the model on some parts, and tests it on others. This process is repeated multiple times to get a reliable performance estimate. Cross-validation helps us understand how well the model works on unseen data and allows us to find the best settings for the model.

As mentioned earlier, the positive and negative classes are somehow imbalanced. Data stratification would be here inappropriate because the minority class will get underrepresented since it has fewer samples.
Therefore, given the size of the dataset, an option would be to undersample the positive class: we train models with data that has the same proportion of positive and negative classes.

Finally, the metrics we used are standard classification metrics such as precision, recall, and F1 score.
Precision measures the rate of true predictions. Recall measures the ability of the model to identify positive/negative comments correctly. F1-score is a metric that combines precision and recall scores (the harmonic mean of the two). All the evaluation is done on the test dataset.

\paragraph{On implementation} We enable to specify a random seed to ensure reproductivity.

\subsection{Naive Bayes}

\begin{figure}[H]
  \centering
  \caption{Confusion matrices and associated metrics}
  
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{nb_mat_no_undersampling.png}
    \caption{No undersampling}
    \label{subfig:no_undersampling}
  \end{subfigure}%
  \hspace*{\fill} % Add horizontal space to center the images
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{nb_mat_undersampling.png}
    \caption{Undersampling}
    \label{subfig:undersampling}
  \end{subfigure}
  
  \vspace{\baselineskip} % Add vertical space between the rows of images
  
  \begin{subfigure}[t]{0.6\textwidth}
    \centering
    \begin{tabular}{cccc}
      & F1 Pos. & F1 Neg. & Accuracy \\
      \hline
      Undersampling & 0.83 & 0.50 & 0.74 \\
      No undersampling & 0.93 & 0.40 & 0.88 \\
    \end{tabular}
    \caption{Scores}
    \label{subfig:scores}
  \end{subfigure}
  
  \label{fig:balancenk}
\end{figure}

We notice that without undersampling, the F1 score is higher for the positive class than for the negative class. The model is biased towards predicting positive sentiment, leading to a higher number of false positives. 
However, with undersampling, the F1-score is lower for the positive class but it more balanced with the negative class. As accuracy is not really relevant in the case of class imbalance, it is probably more appropriate to keep the second model with undersampling.

We are diving into the training phase with looking at learning curves. This is giving us an idea on how well the model generalizes to testing data.

\begin{figure}[H]
  \centering
  \includegraphics[height=7cm]{nb_start_learning_curves.png}
  \caption{Learning curves for Multinomial Naive Bayes with undersampling (with training size in thousands)}
  \label{fig:image_label}
\end{figure}

We can see that the training F1-score is a bit larger than the testing F1-score, which was expected since the model training is done on the training set. We can see that the validation F1-score keeps on decreasing until the end of the training. The model is possibly overfitting. Possibly, training it on a bigger training set could improve performance.

\subsection{K-Nearest neighbors}

To begin with, we delibrately balanced the data, 50\% of positive comments (5000)
and 50\% of negative comments (5000).

By adjusting the $k$, we obtained such result shown in Figure~\ref{fig:KNN1}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
      \begin{axis}[
          xlabel={\textbf{k}},
          ylabel={\textbf{Accuracy}},
          xmin=0, xmax=2100,
          ymin=0.6, ymax=1,
          xtick={0,1000,2000,3000,4000,5000},
          ytick={0.6,0.7,0.8,0.9,1},
          legend pos=north west,
          grid=both,
          grid style={line width=0.2pt, draw=gray!30},
          major grid style={line width=0.4pt,draw=gray!60},
          height=8cm,
      ]
      \addplot[color=blue, mark=*] coordinates {
          (5,0.7606)
          (55,0.8522)
          (500,0.857)
          (1000,0.8332)
          (2000,0.7932)
      };
      \addplot[color=red, mark=square] coordinates {
          (5,0.691)
          (55,0.7964)
          (500,0.857)
          (1000,0.8836)
          (2000,0.908)
      };
      \legend{Negative Comments, Positive Comments}
      \end{axis}
  \end{tikzpicture}
  \caption{k nearest neighbor on balanced data}
  \label{fig:KNN1}
\end{figure}

That was some good results. But in reality, our data is extremely disproportional,
if we apply KNN directly, the algorithm would predict positive almost every time. (Although
a positive comment has less chance to enter the neighborhood of a negative comment,
there are too many of them.) Indeed, we tried and get 0.26 as accuracy for negative comments and 0.94 for
positive comments.

The accuracy in total (on the data set) was good but it was not what we are after. We would like
$(r_p+r_n)/2$ to be big (so in the previous case it was 0.6). $r_{p/n}$ is the accuracy on the positive/negative comments.

To solve this problem, we use our \textbf{balance} parameter introduced before. (recall that balance=1 means total balance, the bigger the less balance)

As the Figure~\ref{fig:balancenk} shows, the balance value would
better stay at 1 (otherwise it damages significantly
the accuracy on the negatives) and we increase k to
remedy the disadvantage of positive prediction.

Where does this disadvantage come from, while the number
of positives and negatives are equal in the training data (after trimming)?
A conjecture is that the number of positive comments in the training data
is significantly less after trimming so
some vocabularies are not even present. In this case,
we can only judge by more basic words and rely on a bigger survey, thus increasing k helps.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek5.png}
    \caption{k=5}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek50.png}
    \caption{k=50}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek500.png}
    \caption{k=500}
  \end{subfigure}
  \caption{Positive comments are labeled as true and negative comments are labeled as false}
  \label{fig:balancenk}
\end{figure}

Now we show a case in reality, our assumption/scenario is that :

The given data on the games is disproportional. Most comments are
positive. So positive comments do not give much information.
The manager then wants to detect then inspect all the negative comments
among all the comments.

Our KNN (with parameters balance=1, k=500 as explained in \ref{subset:KNN}) does this job.
So as shown in Table~\ref{tab:KNN}, we almost covered all negative comments,
the payoff is that the precision of class false is low, meaning
that the manager needs to see as much as 5 comments to read a true negative
comment.

\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
  \hline
  Class & Precision & Recall & F1-Score & Support \\
  \hline
  False (negative comments) & 0.23 & 0.91 & 0.37 & 1789 \\
  True (positive comments) & 0.99 & 0.76 & 0.86 & 22520 \\
  % \hline
  % Macro Avg & 0.61 & 0.83 & 0.61 & 24309 \\
  % Weighted Avg & 0.93 & 0.77 & 0.82 & 24309 \\
  \hline
  \multicolumn{5}{c}{Accuracy = 0.77} \\
  \hline
  \end{tabular}
  \caption{Classification report with undersampling; note that
  we have a smaller support here for KNN than naive bayes,
  this is because of the efficacity problem.}
  \label{tab:KNN}
\end{table}

\subsection{KNN with other representations and distances}

In our KNN above, we used tf-idf representation and cosine distance for reasons already mentioned.
Here we make some experimental justifications by trying out other different options.

\begin{figure}[H]
  \centering
  \begin{minipage}{0.49\textwidth}
  \begin{tabular}{|c|c|c|c|}
  \hline
  & \textbf{BoW} & \textbf{tf-idf} & \textbf{One-hot} \\ \hline
  \textbf{Euclidean} & 0.7642 (1) & 0.7425 (113) & 0.6864 (5) \\ \hline
  \textbf{Cosine} &  0.7725 (1) & 0.7771 (1) & 0.6864 (5) \\ \hline
  \end{tabular}
  \subcaption{The table consists of three columns representing the word representation methods: One Hot Encoding, Bag of Words (BoW), and tf-idf.
  The rows represent the distance measurement techniques: Euclidean distance and cosine distance.
  The table showcases the accuracy values achieved by applying KNN (k=5) with each word representation and distance combination.
  It can be observed that KNN with tf-idf and cosine distance yields the highest accuracy of 0.7104,
  followed closely by KNN with BoW and cosine distance.
  On the other hand, KNN with tf-idf and Euclidean distance exhibits the lowest accuracy of 0.484.}
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
  \includegraphics[width=7cm]{accuracy_plot.png}
  \subcaption{Variation of accuracy with k (KNN, euclidean distance, BoW)}
  \end{minipage}
  \caption{KNN with other options; here we chose to only classify a comment as positive if the game received a rating strictly higher than 7.}
  \label{otherknn}
\end{figure}

Indeed that we see tf-idf combined with cosine distance works the best. Interestingly,
as shown on the right in Figure~\ref{otherknn}, the result is the best when $k=1$.
This is reasonable as it can be interpretated as : euclidean distance only have meaning when
the distance is very small. (Every pair of points tend to have the same distance when the dimension
increases, curse of dimensionality.)

\input{tda.tex}

\section{Insights - Analysis}

\subsection{Challenges encountered}

The most challenging task was to process data since the dataset is very large. We realized how efficient libraries are since their computation time is low compared to when we implement it all by hand.
The implementations are optimized, and the computations are designed for performance. Moreover, libraries such as the ones from sklearn take advantage of parallel computing (executed on multi-core processors). 

Another challenge encountered is that it was not obvious at first how to apply preprocessing. Indeed, we had to think about how much we wanted to remove stopwords or common words. Removing these words might simplify the model, but we might also lose information with it. Another example is the use of negative contractions. It was not obvious if they could have a significant impact or not.

A common obstacle with NLP is that sentences are far from perfect, even after preprocessing is applied. Sentiment classifiers can also struggle when encountering words or phrases that were not present in the training data, so that's why using word embeddings are useful to generalize to unknown terms. 

\subsection{Lessons learned}

The importance of preprocessing is the first lesson we could remember from this project. Indeed, without preprocessing, making a mathematical representation of words does not make any sense since the word embedding space will have a very large dimension, thus making predictions almost impossible.

Then, handling class imbalance. It is easy to classify reviews as positive when the majority of reviews in the training data are positive. Undersampling was a good solution to this problem (with the tradeoff that we lose some precision in prediction). 

\subsection{Limitations and areas of improvement}

Sentiments are subjective, and annotating large datasets with sentiment labels can be challenging due to varying interpretations. Consequently, different customers might assign different sentiment scores, leading to inconsistencies in the training and evaluation data. Additionally, sentiment analysis can be dependent on the context, making it challenging to capture the nuances and sarcasm present in the text. 

Another challenge is the domain adaptation problem because our model is trained on one domain and may struggle to generalize well to other domains due to differences in language usage and sentiment expressions.

We could evaluate the ability of our model to generalize to reviews from other fields (such as movie reviews) and see if it can transfer its knowledge pretty well or not.

Finally, the models we used to represent text data were not sequential for the most part, instead, they focused on the words themselves. Using persistent homology is a way to analyze the structure in a human-force way. Another option would be to use Deep Learning models such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformer-based architectures (e.g., BERT, GPT). These models can capture complex linguistic patterns and contextual information. 

\subsection{Outro}

Throughout the project, we learned how text representation is important for computers to process natural language and different
representations adapt to different tasks. Moreover, we improved our ability to write a good report
and present a project. All thanks to Professor Olivier Schwander.

\input{annexe.tex}

\printbibliography

\end{document}

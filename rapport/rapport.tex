
\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{pgfplots}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage[margin=2cm]{geometry}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    }
\usepackage{biblatex}
\addbibresource{refs.bib}
\usepackage{indentfirst}
\setlength{\parindent}{0.5cm}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Com}{Com}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ker}{Ker}
\newtheorem{theorem}{Theoreme}
\newtheorem*{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Exemple}
\newtheorem{tactic}{Tactic}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbb{I}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\Cn}{\mathscr{C}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\RM}[1]{\paragraph{RM} #1}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\id}{id}

\begin{document}

\tableofcontents

\newpage
\section{Introduction - Motivation}

We were given several datasets from the website ??? which sells a variety of board games. This website collects data for every article, such as reviews and grades posted by the users. 

Our task was to treat and analyze this data to try to gain information from it. One purpose was to classify these reviews and being able to predict the sentiment of review (positive or negative). 

This is a real-world application since the company could automatically gain insight on products by correlating words with grades.
The utilization of word vector representation and dimensionality reduction is useful in establishing meaningful associations between words and the sentiment expressed in reviews. The company could explore the represented space to find trends in what people like or dislike, and therefore potentially increasing sales.

So, what valuable insights can be extracted from these reviews? How can we effectively capture and quantify the sentiments conveyed?
The following report will highlight our methodology to uncover patterns, our data processing approaches and the potential of our findings.  
\newpage
\section{Analysis on data}

We are starting by looking at the grades data, since it will be the criterion to classify reviews as positive or negative.
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{hist_plot_count.png}
    \label{fig:left_image}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{dist_plot_count.png}
    \label{fig:right_image}
  \end{minipage}
\end{figure}

On the density plot, we can notice that the majority of the grades are actually integers. Removing float numbers (or rounding to closest integer) could possibly simplify the model.

On the histogram plot , we can notice that the majority of reviews are actually positive.

It is cautious to already say it could introduce some challenges. Indeed, since grades are quite imbalanced, the model can become biased towards positive reviews. The model may tend to predict positive reviews more frequently, leading to lower accuracy for negative ones and an overall imbalance in predictions. Also, since there are fewer samples from negative reviews, the model may not have enough representative examples to learn the patterns and characteristics of the negative class.

\section{Corpus to vectors}

\subsection{Preprocessing}

Raw text data might contain unwanted or unimportant text due to which our results might not give efficient accuracy, and might make it hard to understand and analyze. So, proper pre-processing must be done on raw data.

We had to remove punctuation, URLs, numbers, accents, stopwords.
Stopwords are commonly used words in a language that are considered to have little or no significance in determining the meaning of a text.
`not' is not a stopword, because it might indicate the opposite sentiment.

We also applied Stemming that reduces words to their root forms. It minimizes the confusion around words that have similar meanings and lowers the complexity of the input space. However, it comes at the cost of throwing information away.

\subsection{One Hot Encoding}

One Hot Encoding is a vector representation of a word in vocabulary, ie the unique list of all the words appearing in the documents.
If the vocabulary size is n, each word in the vocabulary is therefore represented as a vector of size n. It takes binary values: 1 for the corresponding word and 0 otherwise.

\paragraph{On the implementation:}

We can represent the set of sentences as a tensor of shape (a, b, c) ie a matrices (number of sentences) of shape (b,c) where b represents the number of words in the sentence and c the vocabulary size.

\paragraph{Advantages:}

\begin{itemize}
  \item Intuitive and easy to implement
\end{itemize}

\paragraph{Inconvenients:}

\begin{itemize}
  \item Increase in dimensionality: a large vocabulary size implies a large number of columns, taking more memory size, which results in an increase in computational cost. + The matrix is sparse.
  \item Every vector sits in the orthogonal vector space so vectors are perpendicular to each other and are considered independent to each other, which is rarely the case.
  \item High chance of multi-collinearity due to dummy variables, which might affect the performance of the model (cf. Dummy Variable Trap)
\end{itemize}

\subsection{Bag of Words}

\subsection{tf-idf}

The BoW (Bag of Words) model assumes that the importance of a term is directly proportional to
the number of its appearance in the document, this can easily be misleading
when the most common words are `stopwords'. (But it really depends on
the algorithm that we are going to use afterwards, if we do perceptron, it wouldn't matter.)

But the BoW gives vectors with integer features, which may be favorable by some algorithm,
like Naive Bayes below. This being said, the tf-idf embedding gives vectors with features in $\mathbb{Q}$,
which can be dilated to integers.

tf-idf (term frequency - inverse document frequency) first considers
the whole of a corpus, it assumes a term too frequent in the corpus has little
information. Then it considers the importance of a term in a document
as the frequency of the term in this document times a scalar
representing its information in the corpus.

$$
\begin{aligned}
\mathrm{tfidf}(\mathrm{term}) & = \mathrm{tf}(\mathrm{term}) \times \mathrm{idf}(\mathrm{term}) \\
\mathrm{tf}(\mathrm{term}) & = \frac{\# \text { of times term appears in document }}{\# \text { of terms in document }} \\
\mathrm{idf}(\mathrm{term}) & =\ln \left(\frac{\# \text { of documents }}{\# \text { of documents in corpus with term }}\right)
\end{aligned}
$$

Other possible choices:

\begin{itemize}
  \item A term can be many (succesive) words (n-gram) (e.g. to
  take into account of negations before a word)
  \item The exact formulae can be changed while the same idea remains
\end{itemize}

\paragraph{On the implementation} Il faut éviter utiliser \verb|for|, et
utiliser les fonctions numpy à la place; on utilise les matrices sparse pour
gagner de la mémoire et parfois l'éfficacité.

\paragraph{Observations}
\begin{itemize}
  \item Curse of dimensionality
  \item Preprocessing is important
\end{itemize}

\section{Treating vectors}

\subsection{Protocol of the evaluation}

Can specify random seed to ensure reproductivity....

\subsection{Distances}

We can use euclidean distance, or we can use the distance concerning
only the angle between 2 vectors (so the `difference of lengths' of documents
is ignored).

$$
\begin{aligned}
&\text{(cosine similarity)}S(A, B) :=
\cos (\theta)=\frac{A \cdot B}{\|A\|\|B\|}\\
&\text{(cosine distance)}D(A, B) :=1-S(A, B)
\end{aligned}
$$

We see that cosine distance is better in nlp for 1. no curse of dimensionality; 2. no
need to reduce the dimension.

\subsection{Dimensionality reduction}

\begin{itemize}
  \item PCA (not useful for us, every new word (if not a stopword) can not be ignored and their linear combinations have no meaning)
  \item t-SNE (only useful for visulization?)
\end{itemize}

After the reduction of dimensionality, we may do a visulization; but
bad visulization results don't necessarily mean bad classification result.

\subsection{k nearest neighbour}

\paragraph{Algorithm} One predicts the information associated with a vector by the majority
of information associated with its neighbours (within k nearest).
We use tf-idf embedding here.
By simple experience
and raisonning (the length doesn't affect the positivity) as well, we chose cosine distance.

For comments of grades 4 - 7, they are more or less neutral, we can't
say if it is definitely positive or negative as human-being, so less chance
for our more or less naïve algorithm. For this reason we chose
to do our algorithm on comments of extremities.

To begin with, we delibrately balanced the data, 50\% of positive comments (5000)
and 50\% of negative comments (5000).

By adjusting the k, we obtained such result shown in Figure~\ref{fig:knn1}.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
      \begin{axis}[
          xlabel={\textbf{k}},
          ylabel={\textbf{Accuracy}},
          xmin=0, xmax=5500,
          ymin=0.6, ymax=1,
          xtick={0,1000,2000,3000,4000,5000},
          ytick={0.6,0.7,0.8,0.9,1},
          legend pos=north west,
          grid=both,
          grid style={line width=0.2pt, draw=gray!30},
          major grid style={line width=0.4pt,draw=gray!60},
          height=8cm,
      ]
      \addplot[color=blue, mark=*] coordinates {
          (5,0.7606)
          (55,0.8522)
          (500,0.857)
          (1000,0.8332)
          (2000,0.7932)
          (5000,0.6984)
      };
      \addplot[color=red, mark=square] coordinates {
          (5,0.691)
          (55,0.7964)
          (500,0.857)
          (1000,0.8836)
          (2000,0.908)
          (5000,0.9378)
      };
      \legend{Negative Comments, Positive Comments}
      \end{axis}
  \end{tikzpicture}
  \caption{k nearest neighbor on balanced data}
  \label{fig:knn1}
\end{figure}

That was some good results. But in the reality, our data is extremely disporpotional,
if we apply knn directly, the algorithm would predict positive almost everytime. (Although
a positive comment has less chance to enter the neighbourhood of a negtive comment,
there are too many of them.) Indeed, we tried and get 0.26 as accuracy for negative comments and 0.94 for
positive comments.

The accuracy in total (on the data set) was good but it was not what we are after. We would like
$(r_p+r_n)/2$ to be big (so in the previous case it was 0.6). $r_{p/n}$ is the accuracy on the positive/negative comments.

To solve this problem, we add a parameter to our algorithm: \textbf{balance}.
balance is a float >= 1 meaning that we trim the list of data of one label
of more quantity to size = balance$\times$(number of data with another label).

As the Figure~\ref{fig:balancenk} shows, the balance value would
better stay at 1 (otherwise it damages significantly
the accuracy on the negatives) and we increase k to
remedy the disadvantage of positive prediction.

Where does this disadvantage come from, while the number
of positives and negatives are equal in the training data (after trimming).
A conjecture of mine is that, the number of positive comments in the test data
is significantly less than that of the training data so
some vocabularies are not present in the training data. In this case,
we can only judge by more basic words and rely on bigger survey, thus increasing k helps.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek5.png}
    \caption{k=5}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek50.png}
    \caption{k=50}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{balancek500.png}
    \caption{k=500}
  \end{subfigure}
  \caption{Figure with Multiple Images}
  \label{fig:balancenk}
\end{figure}

\paragraph{On the implementation} \begin{itemize}
  \item We favor the nearer information when there is a tie.
  \item Python is not a well typed language, it is so easy to have
  ridiculous bugs.
  \item Cosine distance works much better than euclidean distance.
  \item When the data is disporpotionally labelled, we need to balance the data to ensure performance for prediction of each label.
  \item We sometime use implemented fonctions to improve
  efficacity after having understood the method and
  implemented ourselves.
\end{itemize}

\paragraph{Positive} \begin{itemize}
  \item Easy to implement
\end{itemize}

\paragraph{Negative} \begin{itemize}
  \item Supervised
  \item $\Theta(n)$ for each prediction ($n$ the size of training data), which is
  very slow when we want to use a big training data to ensure better prediction.
\end{itemize}

\subsection{Naive Bayes}

Naive Bayes is a probabilistic classification algorithm.
It is called naive because it assumes that each input variable is independent.

We are looking for:

$$ \max_{y}\mathbb{P}(Y =y \mid X=(x_1, ..., x_n)) $$

using the Naive Bayes formula: $\mathbb{P}(Y \mid X) = \frac{\mathbb{P}(X\mid Y)\cdot \mathbb{P}(Y)}{\mathbb{P}(X)} = \frac{\mathbb{P}(Y)\prod_{i}^{}\mathbb{P}(X_i\mid Y)}{\mathbb{P}(X)}$
by independence,
where $\mathbb{P}(X\mid Y)$ is the likelihood, $\mathbb{P}(X)$ is the evidence, $\mathbb{P}(Y \mid X)$ is the posterior and $\mathbb{P}(Y)$ is the prior.

More precisely, we used Multinomial Naive Bayes for our task because it is suitable for discrete and count-based features, such as word frequencies in text data. The count vectorization of word aligns well with the assumptions of this model. It allows the model to estimate the probability of sentiment classes based on the frequency distribution of words.

\paragraph{Advantages} \begin{itemize}
\item Even though the independence assumption is rarely true, the model is still effective
\item Handles high dimensional data well
\end{itemize}

\paragraph{Inconvenients} \begin{itemize}
\item Estimated probability is often inaccurate because of the naive assumption
\end{itemize}
\subsection{Perceptron}

\paragraph{On the implementation}
% \begin{itemize}
% \end{itemize}

\section{Evaluation of models - Results}

\subsection{Model evaluation}

First of all, the split the labeled dataset into training and testing subsets. The model is trained on the training data and then evaluated on the testing data to measure its performance, in order to assess its capacity to generalize predictions to unknown data.

Then, we used cross-validation for each model. It divides the data into parts, trains the model on some parts, and tests it on others. This process is repeated multiple times to get a reliable performance estimate. Cross-validation helps us understand how well the model works on unseen data and allows us to find the best settings for the model.

As mentioned earlier, the positive and negative classes are somehow imbalanced. Data stratification would be here inappropriate because the minority class will get underrepresented, since it has fewer samples.
Therefore, given the size of the dataset, an option would be to undersample the positive class: we train models with on data that has the same proportion of positive and negative classes.

Finally, the metrics we used are standard classification metrics such as precision, recall and F1 score.
Precision measures the accuracy of positive predictions. Recall measures the ability of the model to identify positive instances correctly. F1-score is a metric that combines precision and recall scores.

\subsection{K-Nearest Neighbors}

\subsection{Naive Bayes}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{class_report_nb_no_strat.png}
    \caption{Classification report with no undersampling}
    \label{fig:image1_label}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{class_report_nb_strat.png}
    \caption{Classification report with undersampling}
    \label{fig:image2_label}
  \end{subfigure}
\end{figure}

We notice that without undersampling, the F1 score is very high for the positive class but it is low for the negative. The model is biased towards predicting positive sentiment, leading to a higher number of false negatives. 

However, with undersampling, the F1-score is lower for the positive class but it as at least balanced with the negative class. This situation is certainly favorable. 

We are diving into the training phase with looking at learning curves. This is giving us an idea on how well the model generalizes to testing data.
\begin{figure}[H]
  \centering
  \includegraphics[height=7cm]{nb_start_learning_curves.png}
  \caption{Learning curves for Multinomial NB with undersampling}
  \label{fig:image_label}
\end{figure}

We can see that the training F1-score is a bit larger than the testing F1-score, which is a sign of overfitting.
It could be attributed to the assumptions of independence made by the algorithm. Naive Bayes assumes that the features are conditionally independent given the class, which may not be the case.

Nonetheless, the difference is not very large, so we might want to stick with it.

\section{Insights - Analysis}

\subsection{Challenges encountered}

The most challenging task was to process data since the dataset is very large. We realized how efficient libraries are, since their computation time is low compared to when we implement it all by hand.
The implementations are optimized, and the computations are designed for performance. Moreover, libraries such as the ones from sklearn take advantage of parallel computing (executed on multi-core processors). 

Another challenge encountered is that it was not obvious at first how to apply preprocessing. Indeed, we had to think about how much we wanted to remove stopwords or common words. Removing these words might simplify the model, but we might also lose information with it. Another example is the use of negative contractions. It was not obvious if they could have a significant impact or not.

A common obstacle with NLP is that sentences are far from being perfect, even after preprocessing is applied. Sentiment classifiers can also struggle when encountering words or phrases that were not present in the training data, so that's why using word embeddings are useful to generalize to unknown terms. 

\subsection{Lessons learned throughout the project}

The importance of preprocessing is a first lesson we could remember from this project. Indeed, without preprocessing, making a mathematical representation of words does not make any sense since the word embedding space will have a very large dimension, thus making predictions almost impossible.

Then, handling class imbalance. It is easy to classify reviews as positive when the majority of reviews in the training data are positive. Undersampling was a good solution to this problem. 

\subsection{Limitations and areas of improvement}

Sentiment is subjective, and annotating large datasets with sentiment labels can be challenging due to varying interpretations. Consequently, different customers might assign different sentiment scores, leading to inconsistencies in the training and evaluation data. Additionally, sentiment analysis can be dependent on the context, making it challenging to capture the nuances and sarcasm present in text. 

Another challenge is the domain adaptation problem, because our model is trained on one domain and may struggle to generalize well to other domains due to differences in language usage and sentiment expressions.

We could evaluate the ability of our model to generalize to reviews from other fields (such as movies reviews) and see if it can transfer its knowledge pretty well or not.

Finally, the models we used to represent text data were not sequential for the most part, instead it focused on the words themselves. An option would be to use Deep Learning models such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformer-based architectures (e.g., BERT, GPT). These models can capture complex linguistic patterns and contextual information. 

\include{tda.tex}

\printbibliography

\end{document}
